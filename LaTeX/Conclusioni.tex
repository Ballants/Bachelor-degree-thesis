\chapter{Conclusioni}

Il Deep Learning (DL) ha notevolmente migliorato l'analisi delle immagini mediche ed è diventato uno strumento indispensabile per medici e operatori sanitari. Tuttavia, gli adversarial attacks ostacolano il corretto funzionamento dei modelli di DL, mettendo in serio pericolo le vite dei pazienti.\\

Nell'elaborato presentato, è stato analizzato il problema degli adversarial attacks sul riconoscimento delle immagini mediche. 

Sono stati condotti una serie di esperimenti con 5 modelli CNN, 4 tipi di attacchi e 2 tecniche di difesa su 2 datasets di immagini mediche, rispettivamente di 2 e 4 classi.

Nessuno dei modelli proposti si è rivelato abbastanza robusto e sicuro da resistere agli adversarial attacks scelti.
Gli attacchi applicati si sono dimostrati, nonostante la loro semplicità, estremamente efficaci. In alcuni casi hanno persino comportato un calo dell'accuracy dei modelli del 100\%.
 
Anche se i metodi di difesa adottati per contrastare gli adversarial attacks hanno ottenuto buoni risultati, che possono ridurre consistentemente il tasso di successo dell'attacco, sono generalmente rivolti ad un tipo specifico di attacco. 
Nel complesso non esiste un metodo di difesa in grado di proteggere un modello da qualsiasi tipo di attacco.
Inoltre, anche se difeso, il modello attaccato non riesce a ripristinare l'accuracy originale che raggiungerebbe su un dataset di immagini pulite in input.\\

Pertanto, la chiave per garantire la sicurezza dei modelli di DL è quella di continuare le ricerche in maniera approfondita sui metodi di adversarial attacks e proporre strategie di difesa più efficienti.